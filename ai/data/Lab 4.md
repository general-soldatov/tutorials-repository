# LAB 4 "ОСНОВЫ ИСПОЛЬЗОВАНИЯ БИБЛИОТЕКИ PYTORCH"

## Target
изучение возможностей библиотеки PyTorch.

## Theory
В данной работе рассмотриваются основы фреймворка глубокого обучения PyTorch.  
Когда необходимо написать искусственную нейронную сеть, решающую определённую задачу, будь то какая-нибудь простая классификация чего-либо или обнаружение лиц людей на видео. Всё, конечно, всегда начинается со сбора данных, а уже потом реализуются модели и проводятся эксперименты.  
Однако специалисты быстро поняли, что писать свои нейронные сети каждый раз с нуля долго и трудозатратно, поэтому придумали так называемые фреймворки – модули, в которых есть функционал, с помощью которого можно быстро и просто решать типовые задачи, и уже с помощью этих средств писать решения к более сложным задачам.
Существует большое количество фремворков глубокого обучения. Разница между ними прежде всего в том, каков общий принцип вычислений. Например, в Caffe и Caffe2 вы пишете код, по сути, составляя его из готовых «кусочков», как в Lego, в TensorFlow и Theano вы сначала объявляете вычислительный граф, потом компилируете его и запускаете (sees.run()), в то время как в Torch и PyTorch вы пишете почти точно так же, как на NumPy, а граф вычислений создаётся только при запуске (то есть существует только во время выполнения, потом он «разрушается»). Keras позволяет как строить блоки, так и компилировать свой граф:

<img width="334" height="250" alt="image" src="https://github.com/user-attachments/assets/712ddd4d-60ce-4c46-af90-b391c401496b" />

## Синтаксис PyTorch
Особенности PyTorch:  
– динамический граф вычислений;  
– удобные модули torch.nn и torchvision для написания нейросеток с минимальными усилиями;  
– в некоторых задачах даже быстрее TensorFlow (но не во всех);  
– легко проводить вычисления на GPU.  
Для использования PyTorch необходимо импортировать модуль:
```py
import torch
```
Рассмотрим, как в PyTorch выполняются операции с векторами. Тензором называется многомерный вектор, то есть есть:
```py
import numpy as np

x = np.array([1, 2, 3]) # вектор = тензор размерности 1 (то есть (1,))
y = np.array([[1, 2, 3], [4, 5, 6]]) # - матрица = тензор размерности 2 (в данном случае тензор (2, 3))
z = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) # "кубик" (3, 3, 3) = тензор размерности 3 (в данном случае (3, 3, 3))
x, y, z
```
Простейшим примером 3-мерного тензора является картинка – это «параллелепипед» из чисел, у коготорого три размерности – высота, ширина и количество каналов, значит это тензор размерности 3. Понятие тензора нужно понимать потому, что в PyTorch мы оперируем переменными типа torch.Tensor (FloatTensor, IntTensor, ByteTensor).  
Типы тензоров в PyTorch:
```py
torch.HalfTensor    # 16 бит, с плавающей точкой
torch.FloatTensor   # 32 бита, с плавающей точкой
torch.DoubleTensor  # 64 бита, с плавающей точкой

torch.ShortTensor   # 16 бит, целочисленный, знаковый
torch.IntTensor     # 32 бита, целочисленный, знаковый
torch.LongTensor    # 64 бита, целочисленный, знаковый

torch.ByteTensor    # 8 бит, целочисленный, беззнаковый
torch.CharTensor    # 8 бит, целочисленный, знаковый
```

## Practical
Рассмотрим простейшие операции создания, манипулирования тензорами:  
Создание тензора:  
```py
a = torch.FloatTensor([1, 2])
a, a.shape

b = torch.FloatTensor([[1, 2, 3], [4, 5, 6]])
b, b.shape

x = torch.FloatTensor(2, 3, 4)
x, x.shape

x = torch.FloatTensor(100)
x, x.shape

x = torch.IntTensor(45, 57, 14, 2)
x.shape
```
Следует обратить внимание, что при создании тензора через указание размерности, он запоняется неопределенными значениями. Для заполнения тензора нулями необходимо вызвать метод zero_( ):
```py
x = torch.IntTensor(3, 2, 4).zero_()
x
```
Изменение размера тензора (аналог np.reshape( )). Метод torch.view() создаёт новый тензор, а не изменяет старый.
```py
b.view(3, 2)
```
В библиотеке реализованы возможности по смене типа тензора:
```py
a = torch.FloatTensor([1.5, 3.2, -7])
a.type_as(torch.IntTensor())

a.type_as(torch.ByteTensor())
```
Следует учитывать, что при .type_as() создаётся новый тензор (старый не меняется), то есть это не in-place операция.  
Индексация в PyTorch аналогична NumPy:
```py
a = torch.FloatTensor([[100, 20, 35], [15, 184, 11], [12, 11, 555]])
a[0, 0], a[0][1]
```
Арифметика и булевы операции работают также, как и в NumPy, но лучше использовать не опреаторы +, -, *, /, а их аналоги
```py
+ -> torch.add()
- -> torch.sub()
* -> torch.mul()
/ -> torch.div()
```
```py
a = torch.FloatTensor([[100, 20, 35], [15, 184, 11], [12, 11, 555]])
b = torch.FloatTensor([[111, 34, 21], [32, 55, 676], [19, 20, 12]])

a + b
```
Вместо представленного выше кода лучше использовать:
```py
a.add(b)
```
В PyTorch реализованы матричные операции:
```py
# Матричное умножение 
z = x.mm(y)
z = torch.mm(x, y)

# Умножение матрицы на вектор
z = x.mv(y)
z = torch.mv(y)

# Скалярное умножение тензоров
z = x.dot(y)
z = torch.dot(x, y)

# Перемножение матриц целями батчами
bz = bx.bmm(by)
bz = torch.bmm(bx, by)
```
Транспонирование матриц (тензора):
```py
a = torch.FloatTensor([[100, 20, 35], [15, 184, 11], [12, 11, 555]])
a, a.t()
```
Все вычисления в PyTorch можно проводить как на CPU, так и на GPU (Graphical Processing Unit) (если она у вас есть). В PyTorch переключение между ними делается очень просто, что является одной из ключевых его особенностей.
```py
x = torch.FloatTensor(1024, 1024).uniform_()
x

x.is_cuda # False
```
Перемещаем тензор на GPU:
```py
x = x.cuda()
x.is_cuda # True
```
В PyTorch на основе библиотеки autograd реализован механизм вычисления градиентов вычислительного графа. Autograd расшифровывается как Automatic Gradients (автоматическое взятие градиентов) – собственно, из названия понятно, что это модуль PyTorch, отвечающий за взятие производных. PyTorch (и любой фреймворк глубокого обучения) может продифференцировать функцию практически любой сложности.  
Импортируем нужный класс:
```py
from torch.autograd import Variable
```
Идея такая: оборачиваем тензор в класс Variable(), получаем тоже тензор, но он имеет способность вычислять себе градиенты. Если а – тензор, обёрнутый в Variable(), то при вызове a.backward() берутся градиенты по всем переменным, от которых зависит тензор a.  
Если вы используете версию pytorch 0.4.0 или более новую, то torch.Tensor и torch.Variable – одно и то же. То есть Вам больше не нужно оборачивать в Variable(), чтобы брать градиенты – они берутся и по Tensor() (torch.Variable() – deprecated).  
Пример вычисления градиентов:
```py
x = torch.FloatTensor(3, 1).uniform_()
w = torch.FloatTensor(3, 3).uniform_()
b = torch.FloatTensor(3, 1).uniform_()

x = Variable(x, requires_grad=True)
w = Variable(w, requires_grad=True)
b = Variable(b, requires_grad=False)

y = (w @ x).add_(b)
loss = y.sum()

# берём градиенты по всем "листьям" - в данном случае это тензоры x, w и b
loss.backward()

x.grad, w.grad, b.grad, y.grad, y
```
Градиенты лежат в поле .grad у тех тензоров (Variable’ов), по которым брали эти градиенты. Градиенты не лежат в той Variable, от котороый они брались.  
Получить тензор из Variable() можно с помощью поля .data: 
```py
x, x.data
```
